{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Sumário<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fazendo-a-limpeza-de-stop-words\" data-toc-modified-id=\"Fazendo-a-limpeza-de-stop-words-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fazendo a limpeza de stop words</a></span></li><li><span><a href=\"#Lematização\" data-toc-modified-id=\"Lematização-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Lematização</a></span><ul class=\"toc-item\"><li><span><a href=\"#NLTK\" data-toc-modified-id=\"NLTK-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>NLTK</a></span></li><li><span><a href=\"#SpaCy\" data-toc-modified-id=\"SpaCy-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>SpaCy</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos Isolados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roc5GYYTUqUq"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CErjZ3mMNuAw",
    "outputId": "88332816-6b2a-488a-e1d5-e10466cf812b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGB8DWIqUuR7"
   },
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "   \n",
    "def exclude_special(s):\n",
    "  rgx_lists = re.findall('[a-zA-Z\\s]', s)\n",
    "  output = ''.join(rgx_lists)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRGGpvAWVE6o"
   },
   "outputs": [],
   "source": [
    "d = {'texto':['façanha do amanhã!', 'teste do õnõté?']}\n",
    "df = pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MLEUgxxUvrH",
    "outputId": "2e4044cf-c454-4f37-8bcc-337bca3f8dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    facanha do amanha!\n",
       "1       teste do onote?\n",
       "Name: texto, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = \"façanha do amanhã\"\n",
    "df['texto'] = df['texto'].apply(strip_accents)\n",
    "df['texto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHaDtA2bXJUJ",
    "outputId": "c4625284-652f-4e5e-e0b5-180c1162b634"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    facanha do amanha\n",
       "1       teste do onote\n",
       "Name: texto, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['texto'].apply(exclude_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WwSH8VMRXkV"
   },
   "source": [
    "## Fazendo a limpeza de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZrTnQG7KDVu",
    "outputId": "69d9a4a7-0e1a-40c4-b3a8-f269defeca81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esse é um texto de exemplo. Há de ter um texto melhor do que este!\n"
     ]
    }
   ],
   "source": [
    "texto_exemplo = \"Esse é um texto de exemplo. Há de ter um texto melhor do que este!\"\n",
    "print(texto_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27Uj-Xk3Ri7p",
    "outputId": "be76d2fb-5a7e-4043-8d8e-7304110fe81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esse e um texto de exemplo. ha de ter um texto melhor do que este!\n"
     ]
    }
   ],
   "source": [
    "# Limpeza inicial ()\n",
    "texto_min = texto_exemplo.lower()\n",
    "texto_n_acentuado = strip_accents(texto_min)\n",
    "print(texto_n_acentuado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6MtwgxyRmXq",
    "outputId": "6e10536b-ad74-449c-8e51-2a7d23977db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esse e um texto de exemplo ha de ter um texto melhor do que este\n"
     ]
    }
   ],
   "source": [
    "texto_s_pontuacao = exclude_special(texto_n_acentuado)\n",
    "print(texto_s_pontuacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ra1q8UqbRnoU",
    "outputId": "7589ea2b-789f-4cef-9bf5-ddf202294e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esse', 'é', 'um', 'texto', 'de', 'exemplo', '.', 'há', 'de', 'ter', 'um', 'texto', 'melhor', 'do', 'que', 'este', '!']\n"
     ]
    }
   ],
   "source": [
    "# declaração de stop_words\n",
    "stop_words = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "word_tokens = nltk.word_tokenize(texto_min)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "kc37rc-WRo5T",
    "outputId": "5de587cb-7e0d-4433-b633-7edb5aa02cb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'texto exemplo . ter texto melhor !'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caracteres_sentenca_filtrada = [word for word in word_tokens if not word in stop_words]\n",
    "sentenca_filtrada = ' '.join(caracteres_sentenca_filtrada)\n",
    "sentenca_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "CI9FFTO0SuZZ",
    "outputId": "2f21ad38-00a2-4f0a-d3f3-f1608d7bbe97"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'esse e um texto de exemplo ha de ter um texto melhor do que este'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_s_pontuacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAdEmy1MS_0x"
   },
   "source": [
    "A sequência ideal para o tratamento:\n",
    "- Remoção de stop_words\n",
    "- Remoção de acentos;\n",
    "- Remoção de pontuação (RegEx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7zGdrVndlDZ"
   },
   "source": [
    "## Lematização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "G44DKHyBfTVp"
   },
   "source": [
    "### NLTK\n",
    "`Não funciona para o português até onde vimos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Kh4KvkHWdewA",
    "outputId": "0f5732a7-d34c-4e29-8bb3-5a291b99a2b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RmeD9NfZd56B"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NK9Pi9gneyw1"
   },
   "outputs": [],
   "source": [
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "t31twBSeezzY",
    "outputId": "e6c0640e-5b1b-4a03-d5d8-cdacd4829037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amigos ---> amigo\n",
      "ammigas ---> ammigas\n",
      "amigo ---> amigo\n",
      "amiga ---> amiga\n",
      "amizade ---> amizade\n"
     ]
    }
   ],
   "source": [
    "# single word lemmatization examples\n",
    "list1 = ['amigos', 'ammigas', 'amigo', 'amiga', 'amizade']\n",
    "\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "4kJ7qoqKfimP"
   },
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "id": "ZKXJuH4nfrSZ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "e38EQna5pjoa",
    "outputId": "06dc4fd9-d088-4153-fdae-da792b0cdd8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-01-23 17:22:02.412531: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pt-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from pt-core-news-sm==3.4.0) (3.4.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (57.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.10.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "id": "k0-gqozEhq6-"
   },
   "outputs": [],
   "source": [
    "load_model = spacy.load('pt_core_news_sm', disable = ['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YjYp8W5FgkhQ"
   },
   "outputs": [],
   "source": [
    "my_text = \"amigos é o plural de amigo e amigas é o plural de amiga. Isso sim é amizade.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "r0NcBkMzjUrC"
   },
   "outputs": [],
   "source": [
    "doc = load_model(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "hidden": true,
    "id": "Y5VRfW3WjcF3",
    "outputId": "b29d8847-14fe-466b-ada3-42f2122697d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'amigo ser o plural de amigo e amiga ser o plural de amiga . isso sim ser amizade .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xThU9_FlSiH"
   },
   "source": [
    "# Modularizando e testando o código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T20:20:24.572678Z",
     "start_time": "2023-02-01T20:19:52.891786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.5.0/pt_core_news_lg-3.5.0-py3-none-any.whl (568.2 MB)\n",
      "     -----                                   79.7/568.2 MB 2.6 MB/s eta 0:03:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T20:19:03.028141Z",
     "start_time": "2023-02-01T20:19:01.642343Z"
    },
    "id": "4eb2swQflejn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences \n",
    "from utils import utils\n",
    "import logging\n",
    "\n",
    "#ToDo: Executar python -m spacy download pt_core_news_lg\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger = utils.get_logger(logger=logger)\n",
    "\n",
    "class text_preparer:\n",
    "    \"\"\"Class dedicated to the data cleaning and preparation\n",
    "    \"\"\"    \n",
    "    def __init__(self, input_path: str, output_path: str) -> None:\n",
    "        \"\"\"Constructor Method of the class text_prepares\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_path : str\n",
    "            Path where the raw file should be readed\n",
    "        output_path : str\n",
    "            Path where the processed file should be written\n",
    "        \"\"\"        \n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.data = pd.read_csv(input_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def __clear_chars__(self, text: str) -> str:\n",
    "        \"\"\"Clear the special chars from the text\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to be treated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Text with no accent or special caracter\n",
    "        \"\"\"        \n",
    "        logging.info(\"\\t Changing caracters with accents\")\n",
    "        # Remoção pontuação\n",
    "        text_without_accents = ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "        \n",
    "        # Limpeza de caracteres especiais\n",
    "        logging.info(\"\\t Cleanning special caracters\")\n",
    "        rgx_lists = re.findall('[a-zA-Z0-9\\s]', text_without_accents)\n",
    "        \n",
    "        # Junção de todos os termos coletados\n",
    "        output = ''.join(rgx_lists) \n",
    "\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def __remove_stop_words__(self, text: str) -> str:\n",
    "        \"\"\"Removes stop words of the text\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to be cleaned\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Text with no stop words\n",
    "        \"\"\"      \n",
    "        logging.info(\"\\rRemovving Stop Words\")\n",
    "        stop_words = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "        word_tokens = nltk.word_tokenize(text)\n",
    "        caracteres_sentenca_filtrada = [word for word in word_tokens if not word in stop_words]\n",
    "        sentenca_filtrada = ' '.join(caracteres_sentenca_filtrada)\n",
    "\n",
    "        return sentenca_filtrada\n",
    "\n",
    "    @classmethod\n",
    "    def __lemmatization__(self, text: str) -> str:\n",
    "        \"\"\"Applies the process of lemmatization, which is responsible for transforming the word in its primitive form\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to have the lemmatization applied\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Text with lemmatization applied\n",
    "        \"\"\"        \n",
    "        # Amigos > Amigo\n",
    "        # Amigo  > Amigo\n",
    "        # Amiga  > Amigo\n",
    "\n",
    "        logging.info(\"\\tAplying lemmatization\")\n",
    "        load_model = spacy.load('pt_core_news_sm', disable = ['parser','ner'])\n",
    "        doc = load_model(text)\n",
    "        output = \" \".join([token.lemma_ for token in doc])\n",
    "        return output\n",
    "\n",
    "    def prepare_text(self) -> None:\n",
    "        \"\"\"Main method responsible for apply all the methods to treat the text data, and stores it in the text_preparer.data            \n",
    "        \"\"\"        \n",
    "        logging.info(\"Starting the data cleaning...\")\n",
    "        \n",
    "        # Copiando para evitar problemas com \n",
    "        df_int = self.data.copy()\n",
    "\n",
    "        # Remoção de nan\n",
    "        df_int = df_int[df_int['input'].notna()]\n",
    "\n",
    "        # Declaração do tipo como string\n",
    "        df_int['x_input'] = df_int['input'].astype(str)\n",
    "\n",
    "        # Tranformação dos caracteres para letras minúsculas;\n",
    "        df_int['x_input'] = df_int['x_input'].str.lower()\n",
    "\n",
    "        # Remoção de palavras irrelevantes (ex: preposições, conjunções, artigos e etc);\n",
    "        df_int['x_input'] = df_int['x_input'].apply(self.__remove_stop_words__)\n",
    "\n",
    "        # Aplicação de Lematização:\n",
    "        df_int['x_input'] = df_int['x_input'].apply(self.__lemmatization__)\n",
    "\n",
    "        # Limpeza de caracteres especiais e remoção pontuação com RegEx\n",
    "        df_int['x_input'] = df_int['x_input'].apply(self.__clear_chars__)\n",
    "\n",
    "        logging.info(\"Data preparation Done!\")\n",
    "\n",
    "        self.data = df_int.copy()\n",
    "  \n",
    "    def export_text(self) -> None:\n",
    "        \"\"\"Exports the text in the specified dir as output_path\n",
    "        \"\"\"        \n",
    "        logging.info(f\"Exporting treated data in {self.output_path}\")\n",
    "        data = self.data.copy()\n",
    "        data.to_csv(self.output_path, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T20:19:15.707728Z",
     "start_time": "2023-02-01T20:19:10.280248Z"
    },
    "id": "x7hEsisRmc5I"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34056\\883773948.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpreparer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_preparer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../data/raw/data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../data/processed/data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreparer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34056\\313901089.py\u001b[0m in \u001b[0;36mprepare_text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# Aplicação de Lematização:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mdf_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_input'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lemmatization__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# Limpeza de caracteres especiais e remoção pontuação com RegEx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m                     \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m                 )\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34056\\313901089.py\u001b[0m in \u001b[0;36m__lemmatization__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\tAplying lemmatization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mload_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pt_core_news_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0menable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp_assist\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "preparer = text_preparer(input_path=\"../data/raw/data.csv\", output_path=\"../data/processed/data.csv\")\n",
    "preparer.prepare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aVGakYOItFKZ"
   },
   "outputs": [],
   "source": [
    "preparer.export_text()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
